# Roots of Equations

The problem treated in this chapter is the ancient problem of finding roots of equations or of systems of equations. The long list of available methods shows the long history of this problem and its continuing importance.

Which method to use depends upon whether one needs all the roots of a particular equation or only a few, whether the roots are real or complex, simple or multiple, whether one has a ready first approximation or not, and so on.

## The iterative method

The **iterative method** solves $x = F(x)$ by the recursion

$$
x_n = F(x_{n-1})
$$

and converges to a root if $|F'(x)| \leq L < 1$. The error $e_n = r - x_n$, where $r$ is the exact root, has the property

$$
e_n \simeq F'(r) e_{n-1}
$$

so that each iteration reduces the error by a factor near $F'(r)$. If $F'(r)$ is near 1 this is slow convergence.

### Solved Problems

#### Problem 25.1

 > Prove that if $r$ is a root of $f(x) = 0$ and if this equation is rewritten in the form $x = F(x)$ in such a way that $|F'(x)| \leq L < 1$ in an interval $I$ centered at $x = r$, then the sequence $x_n = F(x_{n-1})$ with $x_0$ arbitrary but in the interval $I$ has $\lim x_n = r$.

First we find

$$
|F(x) - F(y)| = |F'(\xi)(x - y)| \leq L |x - y|
$$

provided both $x$ and $y$ are close to $r$. Actually it is this Lipschitz condition rather than the more restrictive condition on $F'(x)$ which we need. Now

$$
|x_n - r| = |F(x_{n-1}) - F(r)| \leq L |x_{n-1} - r|
$$

so that, since $L < 1$, each approximation is at least as good as its predecessor. This guarantees that all our approximations are in the interval $I$, so that nothing interrupts the algorithm. Applying the last inequality $n$ times, we have

$$
|x_n - r| \leq L^n |x_0 - r|
$$

and since $L < 1$, $\lim x_n = r$.

The convergence is illustrated in Fig. 25-1. Note that choosing $F(x_{n-1})$ as the next $x_n$ amounts to following one of the horizontal line

![Graphical illustration of the fixed-point iteration method](fig25-2.png)

#### Problem 25.2

> In the year 1225 Leonardo of Pisa studied the equation
> $$
> f(x) = x^3 + 2x^2 + 10x - 20 = 0
> $$
>and produced $x = 1.368808107$. Nobody knows by what method Leonardo found this value but it is a remarkable result for his time. Apply the method of Problem 25.1 to obtain this result.

The equation can be put into the form $x = F(x)$ in many ways. We take $x = F(x) = \frac{20}{x^2 + 2x + 10}$, which suggests the iteration

$$
x_n = \frac{20}{x_{n-1}^2 + 2x_{n-1} + 10}
$$

With $x_0 = 1$ we find $x_1 = \frac{20}{13} \approx 1.538461538$. Continuing the iteration produces the sequence of Table 25.1. Sure enough, on the twenty-fourth round Leonardo’s value appears.

##### Table 25.1

| n  | $x_n$       | n  | $x_n$       |
|----|------------------|----|------------------|
| 1  | 1.538461538      | 13 | 1.368817874      |
| 2  | 1.295019157      | 14 | 1.368803773      |
| 3  | 1.401825309      | 15 | 1.368810031      |
| 4  | 1.354209390      | 16 | 1.368807254      |
| 5  | 1.375298092      | 17 | 1.368808486      |
| 6  | 1.365929788      | 18 | 1.368807940      |
| 7  | 1.370086003      | 19 | 1.368808181      |
| 8  | 1.368241023      | 20 | 1.368808075      |
| 9  | 1.369059812      | 21 | 1.368808122      |
| 10 | 1.368696397      | 22 | 1.368808101      |
| 11 | 1.368857688      | 23 | 1.368808110      |
| 12 | 1.368786102      | 24 | 1.368808107      |

#### Problem 25.3 

> Why is the convergence of the algorithm of the previous problem so slow?

The rate of convergence may be estimated from the relation

$$
e_n = r - x_n = F(r) - F(x_{n-1}) = F'(\xi)(r - x_{n-1}) = F'(\xi) e_{n-1}
$$

which compares the $n$ - th error $e_n$ with the preceding error. As $n$ increases we may take $F'(r)$ as an approximation to $F'(\xi)$, assuming the existence of this derivative. Then $e_n \simeq F'(r) e_{n-1}$. In our example,

$$
F'(r) = -\frac{40(r + 1)}{(r^2 + 2r + 10)^2} \approx -0.44
$$

making each error about $-0.44$ times the one before it. This suggests that two or three iterations will be required for each new correct decimal place, and this is what the algorithm has actually achieved.

---

The $\Delta^2$ **process can accelerate convergence** under some circumstances. It consists of the approximation

$$
r \simeq x_{n+2} - \frac{(\Delta x_{n+1})^2}{\Delta^2 x_n}
$$

which may be derived from the error property given above.

---

#### Problem 25.4 

> Apply the idea of extrapolation to the limit to accelerate the previous algorithm.

This idea may be used whenever information about the character of the error in an algorithm is available. Here we have the approximation $e_n \simeq F'(r) e_{n-1}$. Without knowledge of $F'(r)$ we may still write

$$
r - x_{n+1} \simeq F'(r)(r - x_n)
$$

$$
r - x_{n+2} \simeq F'(r)(r - x_{n+1})
$$

Dividing we find

$$
\frac{r - x_{n+1}}{r - x_{n+2}} \simeq \frac{r - x_n}{r - x_{n+1}}
$$

and solving for the root

$$
r \simeq x_{n+2} - \frac{(x_{n+2} - x_{n+1})^2}{x_{n+2} - 2x_{n+1} + x_n}
= x_{n+2} - \frac{(\Delta x_{n+1})^2}{\Delta^2 x_n}
$$

This is often called the **Aitken $\Delta^2$ process**.

#### Problem 25.5

> Apply extrapolation to the limit to the computation of Problem 25.2.

Using $x_{10}$, $x_{11}$, and $x_{12}$, the formula produces

$$
r \simeq 1.368786102 - \frac{(0.000071586)^2}{-0.000232877} \simeq 1.368808107
$$

which is once again Leonardo’s value. With this extrapolation, only half the iterations are needed. Using it earlier might have made still further economies by stimulating the convergence.

#### Problem 25.6

> Using extrapolation to the limit systematically after each three iterations is what is known as **Steffensen’s method**. Apply this to Leonardo’s equation.

The first three approximations $x_0$, $x_1$, and $x_2$ may be borrowed from Problem 25.2. Aitken’s formula is now used to produce $x_3$:

$$
x_3 = x_2 - \frac{(x_2 - x_1)^2}{x_2 - 2x_1 + x_0} = 1.370813882
$$

The original iteration is now resumed as in Problem 25.2 to produce $x_4$ and $x_5$:

$$
x_4 = F(x_3) = 1.367918090 \quad \quad x_5 = F(x_4) = 1.369203162
$$

Aitken’s formula then yields $x_6$:

$$
x_6 = x_5 - \frac{(x_5 - x_4)^2}{x_5 - 2x_4 + x_3} = 1.368808169
$$

The next cycle brings the iterates

$$
x_7 = 1.368808080 \quad x_8 = 1.368808120
$$

from which Aitken’s formula manages $x_9 = 1.368808108$.
